{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7.2 심층 신경망.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMahnJz9tjOO6VeJwTWbJvz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"33PWS04EosQO"},"source":["# 7.2 심층 신경망_deep neural network, DNN\n","\n","\n","- 딥러닝(deep learning): 딥러닝은 인공신경망과 거의 동의어로 사용되는 경우가 많으며 혹은 심층 신경망을 딥러닝이라고 부름. 심층 신경망은 여러 개의 층을 가진 인공신경망임.\n","\n","\n","- 심층 신경망(deep neural network, DNN): 2개 이상의 층을 포함한 신경망으로 종종\n","다층 인공신경망, 심층 신경망, 딥러닝을 같은 의미로 사용함.\n","\n","- 렐루 함수(ReLU Function): 입력이 양수일 경우 마치 활성화 함수가 없는 것처럼\n","그냥 입력을 통과시키기고 음수일 경우에는 0으로 만드는 함수\n","\n","- 옵티마이저(optimizer): 신경망의 가중치와 절편을 학습하기 위한 알고리즘 또는 방법. 케라스에는 다양한 경사 하강법 알고리즘이 구현되어 있으며 대표적으로 SGD, 네스테로프 모멘텀, RMSprop, Adam 등이 있음"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2TSnarVNo2ML","executionInfo":{"status":"ok","timestamp":1626394899450,"user_tz":-540,"elapsed":3366,"user":{"displayName":"신현지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIZUTd90G8_DQbHqq3OT69KAwbJ4pOLh2elRWwZQ=s64","userId":"11973479220922860143"}},"outputId":"b19aa5fe-074f-49ad-cf41-e20bf7b11158"},"source":["# 라이브러리 호출 및 데이터 준비\n","from tensorflow import keras        # 텐서플로를 사용해 데이터 불러오기\n","from sklearn.model_selection import train_test_split    # 교차 검증으로 성능을 확인\n","\n","(train_input, train_target), (test_input, test_target) =\\\n","    keras.datasets.fashion_mnist.load_data()\n","    # 데이터셋 불러오기, train 과 test set 으로 나누기\n","\n","train_scaled = train_input /255.0\n","    # 픽셀 값을 0~255 범위에서 0~1 사이로 변환\n","\n","train_scaled = train_scaled.reshape(-1, 28*28)\n","    # 28x28 크기의 2차원 배열을 784 크기의 1차원 배열로 펼치기\n","    \n","train_scaled, val_scaled, train_target, val_target = train_test_split(\n","    train_scaled, train_target, test_size=0.2, random_state=42)\n","    # train을 train과 validation으로 나누기"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"326ofHino5ZH","executionInfo":{"status":"ok","timestamp":1626326991428,"user_tz":-540,"elapsed":631,"user":{"displayName":"신현지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIZUTd90G8_DQbHqq3OT69KAwbJ4pOLh2elRWwZQ=s64","userId":"11973479220922860143"}},"outputId":"64a44a53-c226-4312-966c-d00f034f4c7a"},"source":["# 모델에 층을 추가하는 방법 1\n","\n","# 인공 신경망 모델에 층을 2개 추가\n","dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))\n","    # dense1는 은닉층으로, 100개의 뉴런을 가짐, 활성화 함수를 sigmoid로 지정, input 매개변수에서 입력 크기를 784로 지정함.\n","    # 은닉층의 뉴런 개수를 정하는데 특별한 기준은 없으나 몇 개의 뉴런을 두어야 할 지 결정하는 것은 경험이 필요함\n","    # 은닉층의 뉴런 개수는 적어도 출력층의 뉴런보다 많아야 함.\n","\n","dense2 = keras.layers.Dense(10, activation='softmax')\n","    # 출력층, 10개의 클래스를 분류하므로 10개의 뉴런을 두었고 활성화 함수는 softmax로 지정.\n","\n","\n","# 심층 신경망 만들기: 앞서 만든 dense1과 dense2를 Sequential 클래스에 추가함.\n","model1 = keras.Sequential([dense1, dense2])\n","\n","model1.summary() # 케라스 모델의 summary()메서드를 호출하면 층에 대한 유용한 정보를 얻을 수 있음.\n","    # 케라스 모델의 fit 메서드에 훈련 데이터를 주입하면 이 데이터를 잘게 나누어 여러번 걸쳐 경사 하강법 단계를 수행함.\n","    # 미니배치 경사 하강법으로, 기본 미니배치 크기는 32개"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 100)               78500     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                1010      \n","=================================================================\n","Total params: 79,510\n","Trainable params: 79,510\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7suSLyqpJGT","executionInfo":{"status":"ok","timestamp":1626327012291,"user_tz":-540,"elapsed":266,"user":{"displayName":"신현지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIZUTd90G8_DQbHqq3OT69KAwbJ4pOLh2elRWwZQ=s64","userId":"11973479220922860143"}},"outputId":"1307e4b8-590f-412c-96f3-7d1ac0da8375"},"source":["# 모델에 층을 추가하는 방법 2\n","    # Dense1, 2 두 객체를 따로 저장하여 쓸 일이 없기 때문에\n","    # Sequential 클래스의 생성자 안에서 바로 Dense 클래스의 객체를 만드는 경우가 많음\n","    \n","model2 = keras.Sequential([\n","    keras.layers.Dense(100,activation='sigmoid', input_shape=(784,), name='hidden'),\n","    keras.layers.Dense(10, activation='softmax', name='output')], \n","    name='MNIST model')\n","    # model1과 달리 model2는 Sequential 클래스의 name 매개변수로 모델의 이름과 dense층의 이름을 지정.\n","\n","model2.summary() \n","    # 이 방법은 편리하지만 아주 많은 층을 추가하려면 Sequential 클래스 생성자가 매우 길어짐.\n","    # 조건에 따라 층을 추가할 수도 없음. \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"MNIST model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","hidden (Dense)               (None, 100)               78500     \n","_________________________________________________________________\n","output (Dense)               (None, 10)                1010      \n","=================================================================\n","Total params: 79,510\n","Trainable params: 79,510\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d-CA0sBOpPGI","executionInfo":{"status":"ok","timestamp":1626327023635,"user_tz":-540,"elapsed":254,"user":{"displayName":"신현지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIZUTd90G8_DQbHqq3OT69KAwbJ4pOLh2elRWwZQ=s64","userId":"11973479220922860143"}},"outputId":"f71d077d-e0fc-4764-c0ad-041fb17d178f"},"source":["# 모델에 층을 추가하는 방법 3: Sequential 클래스에서 층을 추가할 때 가장 널리 사용하는 방법- add() 메서드\n","model3 = keras.Sequential()\n","model3.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))\n","model3.add(keras.layers.Dense(10, activation='softmax'))\n","\n","model3.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_2 (Dense)              (None, 100)               78500     \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 10)                1010      \n","=================================================================\n","Total params: 79,510\n","Trainable params: 79,510\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DKXR10-zpUG3","executionInfo":{"status":"ok","timestamp":1626327071905,"user_tz":-540,"elapsed":22276,"user":{"displayName":"신현지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIZUTd90G8_DQbHqq3OT69KAwbJ4pOLh2elRWwZQ=s64","userId":"11973479220922860143"}},"outputId":"ea00529a-d3e0-41a1-f26b-779e44e3d1b4"},"source":["# 모델 훈련하기: compile() 메서드\n","model3.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n","\n","model3.fit(train_scaled, train_target, epochs=5)\n","model3.evaluate(val_scaled, val_target)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","1500/1500 [==============================] - 5s 3ms/step - loss: 0.5709 - accuracy: 0.8052\n","Epoch 2/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.4086 - accuracy: 0.8524\n","Epoch 3/5\n","1500/1500 [==============================] - 4s 2ms/step - loss: 0.3726 - accuracy: 0.8665\n","Epoch 4/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3507 - accuracy: 0.8722\n","Epoch 5/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3325 - accuracy: 0.8785\n","375/375 [==============================] - 1s 1ms/step - loss: 0.3558 - accuracy: 0.8689\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.3558274209499359, 0.8689166903495789]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HlUqtx2kq9-_","executionInfo":{"status":"ok","timestamp":1626329059644,"user_tz":-540,"elapsed":257,"user":{"displayName":"신현지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIZUTd90G8_DQbHqq3OT69KAwbJ4pOLh2elRWwZQ=s64","userId":"11973479220922860143"}},"outputId":"8deb2430-c760-4247-813a-7855f56b9f33"},"source":["# 모델 훈련하기_ 케라스에서는 Flatten class\n","    # 패션 MNIST 데이터는 28x28 크기이기 때문에 인공 신경망에 주입하기 위해 넘파이 배열의 reshape() 메서드를 사용해 1차원으로 펼침\n","    # 케라스에서는 Flatten class는 배치자원을 제외하고 나머지 입력차원을 모두 일렬로 펼침.\n","    #   인공 신경망의 성능을 위해 기여하는 바가 없고, 입력층과 은닉층 사이에 추가 하기 떄문에 Flatten 층이라고 함.\n","    #   Flatten층은 다음 코드처럼 입력층 바로 뒤에 추가함.\n","model4 = keras.Sequential()\n","model4.add(keras.layers.Flatten(input_shape=(28,28)))\n","    # Flatten 클래스에 포함된 모델 파라미터는 0개. \n","    # 케라스의 Flatten 층을 신경망 모델에 추가하면 입력값의 차원을 짐작할 수 있는 것이 장점.\n","model4.add(keras.layers.Dense(100, activation='relu'))\n","model4.add(keras.layers.Dense(10, activation='softmax'))\n","\n","model4.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_1 (Flatten)          (None, 784)               0         \n","_________________________________________________________________\n","dense_6 (Dense)              (None, 100)               78500     \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 10)                1010      \n","=================================================================\n","Total params: 79,510\n","Trainable params: 79,510\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pS4brNkC0wF-"},"source":["## ** 렐루 함수 **\n","\n","- 패션 MNIST 데이터는 28x28 크기이기 때문에 인공 신경망에 주입하기 위해 넘파이 배열의 reshape() 메서드를 사용해 1차원으로 펼침\n","- 케라스에서는 Flatten class는 배치자원을 제외하고 나머지 입력차원을 모두 일렬로 펼침.\n","- 인공 신경망의 성능을 위해 기여하는 바가 없고, 입력층과 은닉층 사이에 추가 하기 떄문에 Flatten 층이라고 함.\n","- Flatten층은 다음 코드처럼 입력층 바로 뒤에 추가함."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jc7tnFotq_lT","executionInfo":{"status":"ok","timestamp":1626332043940,"user_tz":-540,"elapsed":257,"user":{"displayName":"신현지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIZUTd90G8_DQbHqq3OT69KAwbJ4pOLh2elRWwZQ=s64","userId":"11973479220922860143"}},"outputId":"89e6650b-86db-47e6-8b2d-92c9dc230e6a"},"source":["model5 = keras.Sequential()\n","model5.add(keras.layers.Flatten(input_shape=(28,28)))\n","model5.add(keras.layers.Dense(100, activation='relu'))\n","model5.add(keras.layers.Dense(10, activation='softmax'))\n","\n","model5.summary()\n","    # Flatten 클래스에 포함된 모델 파라미터는 0개.\n","    # 케라스의 Flatten 층을 신경망 모델에 추가하면 입력값의 차원을 짐작할 수 있는 것이 장점.\n","    # Flatten층에서 입력값 784개의 입력이 그대로 첫 번째 은닉층에 전달되는 것을 확인\n","    # 케라스 API는 입력 데이터에 대한 전처리 과정을 가능한 모델에 포함시킴.\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_8\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten_6 (Flatten)          (None, 784)               0         \n","_________________________________________________________________\n","dense_16 (Dense)             (None, 100)               78500     \n","_________________________________________________________________\n","dense_17 (Dense)             (None, 10)                1010      \n","=================================================================\n","Total params: 79,510\n","Trainable params: 79,510\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gkqzb_xuz6Zv","executionInfo":{"status":"ok","timestamp":1626332125894,"user_tz":-540,"elapsed":22414,"user":{"displayName":"신현지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIZUTd90G8_DQbHqq3OT69KAwbJ4pOLh2elRWwZQ=s64","userId":"11973479220922860143"}},"outputId":"9b3815d0-887e-4e07-d286-fab467b01b26"},"source":["# 모델 훈련: 렐루 함수 적용시, reshpae() 적용하지 않음\n","(train_input, train_target), (test_input, test_target) =\\\n","    keras.datasets.fashion_mnist.load_data()    # 데이터셋 불러오기, train 과 test set 으로 나누기\n","\n","train_scaled = train_input /255.0               # 픽셀 값을 0~255 범위에서 0~1 사이로 변환\n","train_scaled, val_scaled, train_target, val_target = train_test_split(\n","    train_scaled, train_target, test_size=0.2, random_state=42)     # train을 train과 validation으로 나누기\n","\n","model5.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n","model5.fit(train_scaled, train_target, epochs=5)\n","\n","model5.evaluate(val_scaled, val_target)\n","  # 렐루 함수 적용 전보다 적용 후에 정확도가 향상됨. "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","1500/1500 [==============================] - 4s 2ms/step - loss: 0.3133 - accuracy: 0.8905\n","Epoch 2/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3027 - accuracy: 0.8945\n","Epoch 3/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.2941 - accuracy: 0.8976\n","Epoch 4/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.2886 - accuracy: 0.8986\n","Epoch 5/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.2802 - accuracy: 0.9022\n","375/375 [==============================] - 1s 1ms/step - loss: 0.3895 - accuracy: 0.8810\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.3895147740840912, 0.8809999823570251]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DKkLelCF6VdC","executionInfo":{"status":"ok","timestamp":1626332148716,"user_tz":-540,"elapsed":16476,"user":{"displayName":"신현지","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhIZUTd90G8_DQbHqq3OT69KAwbJ4pOLh2elRWwZQ=s64","userId":"11973479220922860143"}},"outputId":"52bcd62f-61d4-4c21-a8a6-2291061f7285"},"source":["# Optimizer를 *Adam* 클래스 설정(기본 RMSprop 사용)\n","\n","# 매개변수 기본값을 사용해 패션 MNIST 모델을 훈련\n","model6 = keras.Sequential()\n","model6.add(keras.layers.Flatten(input_shape=(28,28)))\n","model6.add(keras.layers.Dense(100, activation='relu'))\n","model6.add(keras.layers.Dense(10, activation='softmax'))\n","\n","# 모델 훈련: 렐루 함수 적용시, reshpae() 적용하지 않음\n","(train_input, train_target), (test_input, test_target) =\\\n","    keras.datasets.fashion_mnist.load_data()    # 데이터셋 불러오기, train 과 test set 으로 나누기\n","\n","train_scaled = train_input /255.0               # 픽셀 값을 0~255 범위에서 0~1 사이로 변환\n","train_scaled, val_scaled, train_target, val_target = train_test_split(\n","    train_scaled, train_target, test_size=0.2, random_state=42)     # train을 train과 validation으로 나누기\n","\n","model6.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\n","\n","model6.fit(train_scaled, train_target, epochs=5)\n","\n","model6.evaluate(val_scaled, val_target)\n","  # 환경마다 조금씩 차이가 있지만 여기서는 기본 RMSprop보다 조금 나은 성능을 나타냄"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.5169 - accuracy: 0.8209\n","Epoch 2/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3907 - accuracy: 0.8605\n","Epoch 3/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3476 - accuracy: 0.8733\n","Epoch 4/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3224 - accuracy: 0.8824\n","Epoch 5/5\n","1500/1500 [==============================] - 3s 2ms/step - loss: 0.3021 - accuracy: 0.8905\n","375/375 [==============================] - 1s 1ms/step - loss: 0.3358 - accuracy: 0.8788\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.33583638072013855, 0.8787500262260437]"]},"metadata":{"tags":[]},"execution_count":21}]}]}